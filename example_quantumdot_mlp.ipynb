{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import optax\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.devices())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "n = 20\n",
    "dim = 2\n",
    "batch = 8192\n",
    "beta = 10.0\n",
    "lr = 0.001\n",
    "hidden_sizes = [64, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Coulomb gas\n",
    "\n",
    "We'd like to study thermodynamic property of the classical Coulomb gas, whose Hamiltonian reads\n",
    "\n",
    "$$H= \\sum_{i<j} \\frac{1}{|\\boldsymbol{x}_i - \\boldsymbol{x}_j|} + \\sum_i  \\boldsymbol{x}_i^2 . $$\n",
    "The second term is a harmonic trapping potential. It makes our story easier (no need to consider periodic bondary condition or Ewald sum for long range interaction.)\n",
    "\n",
    "The way to go is to minimize the variationial free energy with respect to a variational probability density $p(\\boldsymbol{x})$\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{\\boldsymbol{x} \\sim p(\\boldsymbol{x})} \\left [\\frac{1}{\\beta}\\ln p(\\boldsymbol{x}) +  H(\\boldsymbol{x}) \\right] \\ge -\\frac{1}{\\beta} \\ln Z, $$ \n",
    "where $Z = \\int d \\boldsymbol{x} e^{-\\beta H}$ and $\\beta$ is the inverse temperature. The equality holds when $p(\\boldsymbol{x}) = e^{-\\beta H}/Z$, i.e., we achieve the exact solution. \n",
    "\n",
    "First thing first, here is the energy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################### \n",
    "def energy_fn(x, n, dim):\n",
    "    i, j = jnp.triu_indices(n, k=1)\n",
    "    rij = jnp.linalg.norm((jnp.reshape(x, (n, 1, dim)) - jnp.reshape(x, (1, n, dim)))[i,j], axis=-1)\n",
    "    return jnp.sum(x**2) + jnp.sum(1/rij)\n",
    "\n",
    "batch_energy = jax.vmap(energy_fn, (0, None, None), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilistic model we will use is the flow model, which involves a change of variables. The coordinate $x$ is expressed as a function of $z$:\n",
    "$$\n",
    "x = g_{\\theta}(z),\n",
    "$$\n",
    "where $\\theta$ represents the parameters of the neural network.\n",
    "\n",
    "The probabilistic transformation can then be written as:\n",
    "$$\n",
    "p_{\\theta}(g_{\\theta}(z)) = q(z) \\left|\\frac{\\partial z}{\\partial g_{\\theta}(z)}\\right|.\n",
    "$$\n",
    "This can be simplified to:\n",
    "$$\n",
    "p(x) = q(z) \\left|\\frac{\\partial z}{\\partial x}\\right|,\n",
    "$$\n",
    "which implies:\n",
    "$$\n",
    "\\ln p(x) = \\ln q(z) - \\ln \\left|\\frac{\\partial x}{\\partial z}\\right|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################### \n",
    "class NeuralNetwork(hk.Module):\n",
    "    def __init__(self, hidden_sizes, n, dim):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.network = hk.nets.MLP(hidden_sizes + [n*dim], \n",
    "                                   activation=jax.nn.softplus, \n",
    "                                   w_init=hk.initializers.TruncatedNormal(0.1), \n",
    "                                   b_init=hk.initializers.TruncatedNormal(1.0))\n",
    "\n",
    "    #========== MLP ==========\n",
    "    def __call__(self, z):\n",
    "        z_flatten = jnp.reshape(z, (self.n*self.dim, ))\n",
    "        x_flatten = self.network(z_flatten)\n",
    "        x = x_flatten.reshape((self.n, self.dim))\n",
    "        return x\n",
    "\n",
    "#################################################################################### \n",
    "def make_flow(hidden_sizes, n, dim):\n",
    "    \n",
    "    def forward_fn(z):\n",
    "        model = NeuralNetwork(hidden_sizes, n, dim)\n",
    "        return model(z)\n",
    "    \n",
    "    flow = hk.transform(forward_fn)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the model.\n",
    "This is the number of parameters in the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = make_flow(hidden_sizes, n, dim)\n",
    "params = flow.init(key, jnp.zeros((n, dim)))\n",
    "\n",
    "from jax.flatten_util import ravel_pytree\n",
    "raveled_params, _ = ravel_pytree(params)\n",
    "\n",
    "print(\"parameters in the flow model: %d\" % raveled_params.size, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logp(flow):\n",
    "    \n",
    "    def logp(z, params):\n",
    "        n, dim = z.shape\n",
    "        x = flow.apply(params, None, z)\n",
    "        logqz = jnp.sum(jax.scipy.stats.norm.logpdf(z))\n",
    "        \n",
    "        z_flatten = z.reshape(-1)\n",
    "        flow_flatten = lambda z: flow.apply(params, None, z.reshape(n, dim)).reshape(-1)\n",
    "        jac = jax.jacfwd(flow_flatten)(z_flatten)\n",
    "        _, logjacdet = jnp.linalg.slogdet(jac)\n",
    "        \n",
    "        logpx = logqz - logjacdet\n",
    "        return logpx, logqz, logjacdet, x\n",
    "    \n",
    "    return logp\n",
    "\n",
    "logp_novmap = make_logp(flow)\n",
    "logp = jax.vmap(logp_novmap, (0, None), (0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is the output of the neural network. The shape of x should be equal to z. The shape of logp should be equal to batchsize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jax.random.normal(key, (batch, n, dim))\n",
    "print(\"z.shape:\", z.shape)\n",
    "\n",
    "logpx, logqz, logjacdet, x = logp(z, params)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"batch logp(x):\", logpx)\n",
    "print(\"batch logq(z):\", logqz)\n",
    "print(\"batch logjacdet(dz/dx):\", logjacdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The gradient of the objective function is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} \n",
    "= \\mathbb{E}_{\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{z})} \n",
    "\\left[   \\nabla_{\\theta} f(g( \\boldsymbol{z}) )\\right]\n",
    ",\n",
    "$$ \n",
    "where $f (\\boldsymbol{x}) =\\frac{1}{\\beta}\\ln p(\\boldsymbol{x}) +  H(\\boldsymbol{x})$.\n",
    "It can be simplified as: \n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} \n",
    "= \\mathbb{E}_{\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{z})} \n",
    "\\left[   \\nabla_{\\theta} \n",
    "\\left( - \\frac{1}{\\beta} \\ln \\left|\\frac{\\partial g_{\\theta}(z)}{\\partial z}\\right| + H(g(z))\n",
    "\\right)\\right].\n",
    "$$ \n",
    "This is known as the Reparametrization gradient estimator. See https://arxiv.org/abs/1906.10652 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss(logp, batch, n, dim, beta):\n",
    "    \n",
    "    def observable_and_lossfn(params, z):\n",
    "        #logpx, logqz, logjacdet, x = logp(z, params)\n",
    "        logpx, _, logjacdet, x = logp(z, params)\n",
    "        \n",
    "        Eloc = batch_energy(x, n, dim)\n",
    "        Floc = logpx / beta + Eloc\n",
    "        \n",
    "        F_mean, F_std = Floc.mean(), jnp.std(Floc)/jnp.sqrt(batch)\n",
    "        E_mean, E_std = Eloc.mean(), jnp.std(Eloc)/jnp.sqrt(batch)\n",
    "        S_mean, S_std = -logpx.mean(), jnp.std(-logpx)/jnp.sqrt(batch)\n",
    "        observable = (F_mean, F_std, E_mean, E_std, S_mean, S_std, x)\n",
    "\n",
    "        gradf_theta = jnp.mean( -logjacdet / beta + Eloc )   \n",
    "        return gradf_theta, observable\n",
    "    \n",
    "    return observable_and_lossfn\n",
    "\n",
    "observable_and_lossfn = make_loss(logp, batch, n, dim, beta)\n",
    "value_and_gradfn = jax.value_and_grad(observable_and_lossfn, argnums=0, has_aux=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here is the training loop. During training we monitor the density and loss histroy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, opt_state, z):\n",
    "    datas, grad_params = value_and_gradfn(params, z)\n",
    "    updates, opt_state = optimizer.update(grad_params, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for i in range(5000):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    \n",
    "    z = jax.random.normal(key, (batch, n, dim))\n",
    "    params, opt_state, datas = update(params, opt_state, z)\n",
    "    \n",
    "    F_mean, F_std, E_mean, E_std, S_mean, S_std, x = datas[1]\n",
    "    loss_history.append(jnp.array([F_mean, F_std, E_mean, E_std, S_mean, S_std]))\n",
    "    print(\"epoch: %04d    F: %.6f (%.6f)    E: %.6f (%.6f)    S: %.6f (%.6f)\"\n",
    "          %(i, F_mean, F_std, E_mean, E_std, S_mean, S_std))\n",
    "\n",
    "    plot_x = jnp.reshape(x, (batch*n, dim)) \n",
    "    plot_z = jnp.reshape(z, (batch*n, dim))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6), dpi=300)\n",
    "    plt.title(\"epoch: %04d    F: %.6f (%.6f)    E: %.6f (%.6f)    S: %.6f (%.6f)\"\n",
    "          %(i, F_mean, F_std, E_mean, E_std, S_mean, S_std), fontsize=16)\n",
    "    plt.axis('off')\n",
    "    #====== plot x ======\n",
    "    plt.subplot(1, 3, 1)\n",
    "    H, xedges, yedges = np.histogram2d(plot_x[:, 0], plot_x[:, 1], bins=100, \n",
    "                                       range=((-4, 4), (-4, 4)), density=True)\n",
    "    plt.imshow(H, interpolation=\"nearest\", \n",
    "               extent=(xedges[0], xedges[-1], yedges[0], yedges[-1]), cmap=\"inferno\")\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "    #====== plot z ======\n",
    "    plt.subplot(1, 3, 2)\n",
    "    H, xedges, yedges = np.histogram2d(plot_z[:, 0], plot_z[:, 1], bins=100, \n",
    "                                       range=((-4, 4), (-4, 4)), density=True)\n",
    "    plt.imshow(H, interpolation=\"nearest\", \n",
    "               extent=(xedges[0], xedges[-1], yedges[0], yedges[-1]), cmap=\"inferno\")\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "    #====== plot loss ======\n",
    "    plt.subplot(1, 3, 3)\n",
    "    y = np.reshape(np.array(loss_history), (-1, 6))\n",
    "    plt.errorbar(np.arange(i+1), y[:, 0], yerr=y[:, 1], marker='o', capsize=8)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('variational free energy')\n",
    "    plt.pause(0.0001)\n",
    "    \n",
    "print(loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see structure emerges from training? \n",
    "Yes! It is called [Wigner molecule](https://en.wikipedia.org/wiki/Wigner_crystal). Physicists had already studied [the ground state of Wigner crystals for such small clusters](https://www.sciencedirect.com/science/article/abs/pii/S0749603683710268). the interesting result is that for the case of six electrons, there are five electrons arranged around a single electron at the center, which is actually what we get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
