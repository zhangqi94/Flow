{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import optax\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.devices())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "n = 20\n",
    "dim = 2\n",
    "batch = 8192\n",
    "beta = 10.0\n",
    "lr = 0.001\n",
    "\n",
    "nvp_depth = 8\n",
    "mlp_width = 32\n",
    "mlp_depth = 2\n",
    "\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################### \n",
    "def energy_fn(x, n, dim):\n",
    "    i, j = np.triu_indices(n, k=1)\n",
    "    r_ee = jnp.linalg.norm((jnp.reshape(x, (n, 1, dim)) - jnp.reshape(x, (1, n, dim)))[i,j], axis=-1)\n",
    "    return jnp.sum(1/r_ee) + jnp.sum(x**2)\n",
    "\n",
    "batch_energy = jax.vmap(energy_fn, (0, None, None), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(hk.Module):\n",
    "    \"\"\"\n",
    "        Real-valued non-volume preserving (real NVP) transform. \n",
    "        The implementation follows the paper \"arXiv:1605.08803.\"\n",
    "    \"\"\"\n",
    "    def __init__(self, maskflow, nvp_depth, mlp_width, mlp_depth, event_size):\n",
    "        super().__init__()\n",
    "        self.maskflow = maskflow\n",
    "        self.nvp_depth = nvp_depth\n",
    "        self.event_size = event_size\n",
    "        \n",
    "        self.fc_mlp = [hk.nets.MLP([mlp_width]*mlp_depth, \n",
    "                        activation=jax.nn.tanh,\n",
    "                        activate_final=True)\n",
    "                        for _ in range(nvp_depth)]\n",
    "        \n",
    "        self.fc_lin = [hk.Linear(event_size * 2,\n",
    "                        w_init=hk.initializers.TruncatedNormal(stddev=0.1), \n",
    "                        b_init=hk.initializers.TruncatedNormal(stddev=1.0, mean=1.0, lower=0.1, upper=4.0))\n",
    "                        for _ in range(nvp_depth)]\n",
    "        \n",
    "        self.zoom = hk.get_parameter(\"zoom\", [event_size, ], \n",
    "                        init=hk.initializers.Constant(-1.5), dtype=jnp.float64)\n",
    "\n",
    "    ####################################################################################\n",
    "    def coupling_forward(self, x1, x2, l):\n",
    "        ## get shift and log(scale) from x1\n",
    "        shift_and_logscale = self.fc_lin[l](self.fc_mlp[l](x1))\n",
    "        shift, logscale = jnp.split(shift_and_logscale, 2, axis=-1)\n",
    "\n",
    "        logscale = jnp.where(self.maskflow[l], 0, jax.nn.tanh(logscale)*self.zoom)\n",
    "        \n",
    "        ## transform: y2 = x2 * scale + shift\n",
    "        y2 = x2 * jnp.exp(logscale) + shift\n",
    "        ## calculate: logjacdet for each layer\n",
    "        sum_logscale = jnp.sum(logscale)\n",
    "        \n",
    "        return y2, sum_logscale\n",
    "    \n",
    "    ####################################################################################   \n",
    "    def __call__(self, x):\n",
    "        #========== Real NVP (forward) ==========\n",
    "        n, dim = x.shape  \n",
    "        \n",
    "        ## initial x and logjacdet\n",
    "        x_flatten = jnp.reshape(x, (n*dim, ))\n",
    "        logjacdet = 0\n",
    "        \n",
    "        for l in range(self.nvp_depth):\n",
    "            ## split x into two parts: x1, x2\n",
    "            x1 = jnp.where(self.maskflow[l], x_flatten, 0)\n",
    "            x2 = jnp.where(self.maskflow[l], 0, x_flatten)\n",
    "            \n",
    "            ## get y2 from fc(x1), and calculate logjacdet = sum_l log(scale_l)\n",
    "            y2, sum_logscale = self.coupling_forward(x1, x2, l)\n",
    "            logjacdet += sum_logscale\n",
    "\n",
    "            ## update: [x1, x2] -> [x1, y2]\n",
    "            x_flatten = jnp.where(self.maskflow[l], x_flatten, y2)\n",
    "            \n",
    "        x = jnp.reshape(x_flatten, (n, dim))\n",
    "        return x , logjacdet\n",
    "\n",
    "####################################################################################\n",
    "def get_maskflow(key, nvp_depth, event_size):\n",
    "    \n",
    "    mask1 = jnp.arange(0, jnp.prod(event_size)) % 2 == 0\n",
    "    mask1 = (jnp.reshape(mask1, event_size)).astype(bool)\n",
    "    mask2 = jnp.arange(0, jnp.prod(event_size)) % 2 == 1\n",
    "    mask2 = (jnp.reshape(mask2, event_size)).astype(bool)\n",
    "    \n",
    "    maskflow = []\n",
    "    for ii in range(nvp_depth):\n",
    "        if   ii % 2 == 1: mask = mask1\n",
    "        elif ii % 2 == 0: mask = mask2\n",
    "        maskflow += [mask]\n",
    "    return maskflow\n",
    "\n",
    "# def get_maskflow(key, nvp_depth, event_size):\n",
    "    \n",
    "#     mask1 = jnp.concatenate([jnp.ones(event_size//2), jnp.zeros(event_size//2)])\n",
    "#     mask1 = (jnp.reshape(mask1, event_size)).astype(bool)\n",
    "#     mask2 = jnp.concatenate([jnp.zeros(event_size//2), jnp.ones(event_size//2)])\n",
    "#     mask2 = (jnp.reshape(mask2, event_size)).astype(bool)\n",
    "    \n",
    "#     maskflow = []\n",
    "#     for ii in range(nvp_depth):\n",
    "#         if   ii % 2 == 1: mask = mask1\n",
    "#         elif ii % 2 == 0: mask = mask2\n",
    "#         maskflow += [mask]\n",
    "#     return maskflow\n",
    "\n",
    "#################################################################################### \n",
    "def make_flow(nvp_depth, mlp_width, mlp_depth, n, dim):\n",
    "\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    maskflow = get_maskflow(key, nvp_depth, n*dim)\n",
    "    print(jnp.array(maskflow).astype(jnp.int32))\n",
    "    \n",
    "    def forward_fn(z):\n",
    "        model = RealNVP(maskflow, nvp_depth, mlp_width, mlp_depth, n*dim)\n",
    "        return model(z)\n",
    "    \n",
    "    flow = hk.transform(forward_fn)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = make_flow(nvp_depth, mlp_width, mlp_depth, n, dim)\n",
    "params = flow.init(key, jnp.zeros((n, dim)))\n",
    "\n",
    "from jax.flatten_util import ravel_pytree\n",
    "raveled_params, _ = ravel_pytree(params)\n",
    "\n",
    "print(\"parameters in the flow model: %d\" % raveled_params.size, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logp(flow):\n",
    "    \n",
    "    def logp(z, params):\n",
    "        n, dim = z.shape\n",
    "        x, logjacdet = flow.apply(params, None, z)\n",
    "        logqz = jnp.sum(jax.scipy.stats.norm.logpdf(z))\n",
    "        \n",
    "        # z_flatten = z.reshape(-1)\n",
    "        # flow_flatten = lambda z: flow.apply(params, None, z.reshape(n, dim))[0].reshape(-1)\n",
    "        # jac = jax.jacfwd(flow_flatten)(z_flatten)\n",
    "        # _, logjacdet = jnp.linalg.slogdet(jac)\n",
    "        \n",
    "        logpx = logqz - logjacdet\n",
    "        return logpx, logqz, logjacdet, x\n",
    "    \n",
    "    return logp\n",
    "\n",
    "logp_novmap = make_logp(flow)\n",
    "logp = jax.vmap(logp_novmap, (0, None), (0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jax.random.normal(key, (batch, n, dim))\n",
    "print(\"z.shape:\", z.shape)\n",
    "\n",
    "logpx, logqz, logjacdet, x = logp(z, params)\n",
    "print(\"batch logp(x):\", logpx)\n",
    "print(\"batch logq(z):\", logqz)\n",
    "print(\"batch logjacdet(dz/dx):\", logjacdet)\n",
    "print(jnp.mean(-logpx))\n",
    "print(jnp.mean(-logqz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_x = x.reshape(-1, 2)\n",
    "fig = plt.figure(figsize=(3, 3), dpi = 300)\n",
    "plt.scatter(plot_x[:, 0], plot_x[:, 1], alpha=0.1, s=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss(logp, batch, n, dim, beta):\n",
    "    \n",
    "    def observable_and_lossfn(params, z):\n",
    "        logpx, logqz, logjacdet, x = logp(z, params)\n",
    "        \n",
    "        Eloc = batch_energy(x, n, dim)\n",
    "        Floc = logpx / beta + Eloc\n",
    "        \n",
    "        F_mean, F_std = Floc.mean(), jnp.std(Floc)/jnp.sqrt(batch)\n",
    "        E_mean, E_std = Eloc.mean(), jnp.std(Eloc)/jnp.sqrt(batch)\n",
    "        S_mean, S_std = -logpx.mean(), jnp.std(-logpx)/jnp.sqrt(batch)\n",
    "        observable = (F_mean, F_std, E_mean, E_std, S_mean, S_std, x)\n",
    "\n",
    "        gradf_theta = jnp.mean( -logjacdet / beta + Eloc )   \n",
    "        #gradf_theta = jnp.mean( logpx / beta + Eloc )\n",
    "        return gradf_theta, observable\n",
    "    \n",
    "    return observable_and_lossfn\n",
    "\n",
    "observable_and_lossfn = make_loss(logp, batch, n, dim, beta)\n",
    "value_and_gradfn = jax.value_and_grad(observable_and_lossfn, argnums=0, has_aux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, opt_state, z):\n",
    "    datas, grad_params = value_and_gradfn(params, z)\n",
    "    updates, opt_state = optimizer.update(grad_params, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for i in range(epochs):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    \n",
    "    z = jax.random.normal(key, (batch, n, dim)) \n",
    "    params, opt_state, datas = update(params, opt_state, z)\n",
    "    \n",
    "    F_mean, F_std, E_mean, E_std, S_mean, S_std, x = datas[1]\n",
    "    loss_history.append(jnp.array([F_mean, F_std, E_mean, E_std, S_mean, S_std]))\n",
    "    print(\"epoch: %04d    F: %.6f (%.6f)    E: %.6f (%.6f)    S: %.6f (%.6f)\"\n",
    "          %(i, F_mean, F_std, E_mean, E_std, S_mean, S_std))\n",
    "\n",
    "    plot_x = jnp.reshape(x, (batch*n, dim)) \n",
    "    plot_z = jnp.reshape(z, (batch*n, dim))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6), dpi=300)\n",
    "    plt.title(\"epoch: %04d    F: %.6f (%.6f)    E: %.6f (%.6f)    S: %.6f (%.6f)\"\n",
    "          %(i, F_mean, F_std, E_mean, E_std, S_mean, S_std), fontsize=16)\n",
    "    plt.axis('off')\n",
    "    #====== plot x ======\n",
    "    plt.subplot(1, 3, 1)\n",
    "    H, xedges, yedges = np.histogram2d(plot_x[:, 0], plot_x[:, 1], bins=100, \n",
    "                                       range=((-4, 4), (-4, 4)), density=True)\n",
    "    plt.imshow(H, interpolation=\"nearest\", \n",
    "               extent=(xedges[0], xedges[-1], yedges[0], yedges[-1]), cmap=\"inferno\")\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "    #====== plot z ======\n",
    "    plt.subplot(1, 3, 2)\n",
    "    H, xedges, yedges = np.histogram2d(plot_z[:, 0], plot_z[:, 1], bins=100, \n",
    "                                       range=((-4, 4), (-4, 4)), density=True)\n",
    "    plt.imshow(H, interpolation=\"nearest\", \n",
    "               extent=(xedges[0], xedges[-1], yedges[0], yedges[-1]), cmap=\"inferno\")\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "    #====== plot loss ======\n",
    "    plt.subplot(1, 3, 3)\n",
    "    y = np.reshape(np.array(loss_history), (-1, 6))\n",
    "    plt.errorbar(np.arange(i+1), y[:, 0], yerr=y[:, 1], marker='o', capsize=8)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('variational free energy')\n",
    "    plt.pause(0.0001)\n",
    "    \n",
    "print(loss_history[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
